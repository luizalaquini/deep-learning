{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Passo a passo:\n",
        "\n",
        "1. Carregar o dataset Boston Housing\n",
        "O dataset Boston Housing contém dados sobre preços de casas, com 13 variáveis preditoras (como número de quartos, índice de criminalidade, etc.) e o preço médio como variável alvo.\n",
        "\n",
        "2. Normalizar os dados de entrada:\n",
        "Os dados de entrada são normalizados usando o StandardScaler do sklearn. A normalização garante que todas as features tenham uma média próxima de 0 e um desvio padrão de 1, o que ajuda a melhorar a convergência durante o treinamento do modelo.\n",
        "\n",
        "3. Definir a entrada do modelo\n",
        "A entrada do modelo é criada usando a API funcional do Keras. O formato esperado é (13,), pois existem 13 features em cada amostra do dataset.\n",
        "\n",
        "4. Criar camadas ocultas\n",
        "As camadas ocultas são conectadas à camada de entrada. Cada camada usa a ativação relu, que é amplamente utilizada em redes neurais devido à sua simplicidade e eficiência.\n",
        "\n",
        "5. Definir a camada de saída\n",
        "A camada de saída é uma camada densa com apenas um neurônio, pois o problema é de regressão (prever o preço de uma casa). Não há função de ativação, pois o modelo deve prever valores contínuos.\n",
        "\n",
        "6. Criar o modelo\n",
        "Com a API funcional, o modelo é criado conectando as camadas de entrada e saída.\n",
        "\n",
        "\n",
        "# Resumo:\n",
        "\n",
        "- Arquitetura funcional do Keras: Foi usada para criar um modelo flexível e eficiente.\n",
        "- Normalização: Garante a uniformidade dos dados de entrada.\n",
        "- Monitoramento da validação: Ajuda a identificar overfitting durante o treinamento.\n",
        "- Métricas: O erro quadrático médio mede o desempenho na previsão de preços.\n",
        "Essa abordagem demonstra como usar a API funcional do Keras para construir, treinar e avaliar modelos para problemas de regressão."
      ],
      "metadata": {
        "id": "PbqIhRVEo19P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exemplo de uso da arquitetura com \"Model\" do Keras"
      ],
      "metadata": {
        "id": "c6kTWLvOQMKo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3rpkW1NqP2G7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b96cb15-79a6-451b-eec8-adffbe917631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "\u001b[1m57026/57026\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Importando as bibliotecas necessárias\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from keras.datasets import boston_housing\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Definir a seed para reprodutibilidade\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "tensorflow.random.set_seed(seed)\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "initializer = tensorflow.keras.initializers.GlorotUniform(seed=seed)\n",
        "\n",
        "tensorflow.config.experimental.enable_op_determinism()\n",
        "\n",
        "# Carregando o conjunto de dados Boston Housing\n",
        "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
        "\n",
        "# Normalizando os dados de entrada usando StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo a entrada do modelo\n",
        "input_layer = Input(shape=(x_train.shape[1],))\n",
        "\n",
        "# Definindo as camadas do modelo\n",
        "hidden_layer1 = Dense(64, activation='relu')(input_layer)\n",
        "hidden_layer2 = Dense(32, activation='relu')(hidden_layer1)\n",
        "output_layer = Dense(1)(hidden_layer2)\n",
        "\n",
        "# Criando o modelo\n",
        "model = Model(inputs=input_layer, outputs=output_layer)"
      ],
      "metadata": {
        "id": "_K9vKAvmQj7A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilando o modelo\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Dividindo os dados em conjunto de treinamento e validação\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Treinando o modelo\n",
        "model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Avaliando o modelo com dados de teste\n",
        "test_loss = model.evaluate(x_test, y_test)\n",
        "print(f'Erro quadrático médio no conjunto de teste: {test_loss}')"
      ],
      "metadata": {
        "id": "AE9bzGKnQp0p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}